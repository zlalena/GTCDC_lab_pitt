{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6557b8fe",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aaf77d",
   "metadata": {},
   "source": [
    "# Reconstructing Outdoor Environments for Physical AI Simulation with 3D Gaussian Splatting in NVIDIA Isaac Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a806ec0",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "<br>\n",
    "In this notebook we demonstrate how to reconstruct an outdoor scene. This includes the following sections:\n",
    "\n",
    "[Part 1: Reconstructing a Scene with Gaussian Splatting](#part-1-reconstructing-a-scene-with-gaussian-splatting)<br>\n",
    "    &emsp;&emsp;1. [Gather Images](#gather-images)<br>\n",
    "    &emsp;&emsp;2. [Run Structure from Motion (COLMAP)](#run-structure-from-motion-colmap)<br>\n",
    "    &emsp;&emsp;3. [fVDB Reality Capture](#fvdb-reality-capture)<br>\n",
    "    &emsp;&emsp;&emsp;&emsp;a) [Setup and Imports](#setup-and-imports) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;b) [Visualize a Sfm Scene](#visualize-a-sfm-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;c) [Preprocess Scene](#preprocess-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;d) [Train a Gaussian Splatting Scene](#train-a-gaussian-splatting-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;e) [Visualize a Gaussian Splatting Scene](#visualize-a-gaussian-splatting-scene) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;f) [Convert to a Mesh](#convert-to-a-mesh) <br>\n",
    "    &emsp;&emsp;&emsp;&emsp;g) [Create Isaac Sim Ready Files](#create-isaac-sim-ready-files)<br>\n",
    "[Part 2: Creating an Isaac Sim Scene](#part-2-creating-an-isaac-sim-scene)<br>\n",
    "    &emsp;&emsp;6. [Running Isaac Sim](#running-isaac-sim)<br>\n",
    "    &emsp;&emsp;7. [Import the Assets](#import-the-assets)<br>\n",
    "    &emsp;&emsp;8. [Scene Setup](#scene-setup)<br>\n",
    "    &emsp;&emsp;9. [Optional: Splat Color Editing](#optional-splat-color-editing)<br>\n",
    "    &emsp;&emsp;10. [Save Scene](#save-scene)<br>\n",
    "    &emsp;&emsp;11. [Isaac Lab and Robot Locomotion](#isaac-lab-and-robot-locomotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ef886-79a4-4e9c-84d1-feaeb6ae1678",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this lab we will learn to how to reconstruct an outdoor scene to test robots using NVIDIA fVDB framework and NVIDIA Omniverse NuRec rendering in Isaac Sim. We will walk through core reconstruction and rendering technologies, with a step-by-step workflow for simulating an entire outdoor environment for testing any robot. We will learn how to position images using structure from motion, train a 3D Gaussian splatting scene, extract 3D mesh, and convert to USD for simulation in Isaac Sim. This lab is split into two parts. In [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting), we will learn how to gather good data, use a SfM tool, and train a Gaussian splatting scene. In [Part 2](#Part-2-creating-an-isaac-sim-scene), we will switch focus to robotic simulation. We will import the files from [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting) into Isaac Sim and use Isaac Lab to move a [Spot](https://bostondynamics.com/products/spot/) robot around the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1771364",
   "metadata": {},
   "source": [
    "## Part 1: Reconstructing a Scene with Gaussian Splatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53641d",
   "metadata": {},
   "source": [
    "### Gather Images \n",
    "Your reconstruction can only be as good as the data you start with, because of this it's crucial to have good images. For outdoor scenes there are some best practices you can follow to get good results. \n",
    "* If you have one main object or area you're focusing on it is recommended to circle or orbit it with a camera equipped aerial vehicle. More details on object centric collection can be found in [nerf_dataset_tips.md]( https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md).\n",
    "* For larger outdoor scenes with no one specific focus, either overlapping circular orbits or traditional oblique mapping flight lines produce good quality results. An example flight mode for a popular drone model can be found [here](https://enterprise-insights.dji.com/blog/smart-oblique-capture).\n",
    "\n",
    "In this lab, we will show an example of an orbit collection using a video captured during flight. Frames are extracted every second or so for training. [FFmpeg](https://www.ffmpeg.org/), an open-source software for audio and video file processing, can be used to save individual frames from a video. Below is an example of how you can use FFmpeg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2efacd",
   "metadata": {},
   "source": [
    "<table style=\"width:80%;\">\n",
    "  <tr>\n",
    "    <td style=\"width:50%; text-align:center;\">\n",
    "      <img src=\"images/safety_park_camera_poses.png\" alt=\"First Image\" style=\"width:100%;\">\n",
    "      <div>Camera positions for a scene with a single area of focus. Cameras point towards the center of the scene wile orbiting around it. This scene is Safety Park, we will be using this scene for the rest of the lab.</div>\n",
    "    </td>\n",
    "    <td style=\"width:50%; text-align:center;\">\n",
    "      <img src=\"images/civil_air_patrol_camera_poses.png\" alt=\"Second Image\" style=\"width:100%;\">\n",
    "      <div>Camera positions for a large scene with no specific single area of focus. In this case the camera makes many orbits, covering about 7 square kilometers.</div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49150106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory to write images to\n",
    "!mkdir -p ../../Results/safety_park/images_raw/\n",
    "\n",
    "# save 1 frame per second from the original video to the **images_raw** folder as images.\n",
    "!ffmpeg -i ../../Data/safety_park/safety_park.webm -vf fps=1 ../../Results/safety_park/images_raw/output_frame_%04d.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba65225",
   "metadata": {},
   "source": [
    "For this lab we have curated data for you to use. We will be reconstructing Safety Park, a small pseudo town used for first responder training. Let's view the original video and some of the images of this park that we will use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39124830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play video\n",
    "! vlc ../../Data/safety_park/safety_park.webm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View images\n",
    "from IPython.display import Image\n",
    "Image(filename=\"../../Data/safety_park/images_raw/000059.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e29ec-6690-4ff1-928c-eee1146ad248",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"../../Data/safety_park/images_raw/000114.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6ff48",
   "metadata": {},
   "source": [
    "### Run Structure from Motion (COLMAP)\n",
    "Many radiance field rendering methods, including 3D Gaussian Splatting, require the camera positions and a sparse point cloud of the scene for initialization. We currently have a folder of raw images, but no corresponding camera location or pose information. We can use a structure from motion tool (SfM) to estimate where the camera was for each image and to create a sparse point cloud of the scene. A commonly used SfM tool is [COLMAP](https://colmap.github.io/install.html), which we will use in combination with GLOMAP. [GLOMAP](https://github.com/colmap/glomap) replaces COLMAP's mapper step, focusing on global positioning rather than incremental, and can run 10 or even 100 times faster than COLMAP's. Below we have provided example commands. There's no need to run them here as we have provided the result from an existing SfM run. You can run these commands on your own data by downloading COLMAP and GLOMAP and changing the **/Data/Path** to a directory that contains the **images_raw** folder of your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example COLMAP & GLOMAP commands\n",
    "\n",
    "# Run the feature extract to identify key points\n",
    "!colmap feature_extractor \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --ImageReader.camera_model PINHOLE \\\n",
    "    --ImageReader.single_camera 1 \\\n",
    "    --SiftExtraction.use_gpu 1 \\\n",
    "    --SiftExtraction.max_image_size 7096 \\\n",
    "    --SiftExtraction.max_num_features 20000 \\\n",
    "    --SiftExtraction.num_threads 14\n",
    "\n",
    "# Mature the features across images\n",
    "!colmap exhaustive_matcher \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --SiftMatching.use_gpu 1 \\\n",
    "    --SiftMatching.max_num_matches 60000 \\\n",
    "    --SiftMatching.guided_matching=true\n",
    "\n",
    "# Create a sparse 3D point cloud of the scene using GLOMAP's global mapper\n",
    "!glomap mapper \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --output_path /Data/Path/sparse \\\n",
    "    --GlobalPositioning.use_gpu 1 \\\n",
    "    --BundleAdjustment.use_gpu 1\n",
    "\n",
    "# Align 3d model by applying transformations\n",
    "!colmap model_aligner \\\n",
    "    --input_path /Data/Path/sparse/0 \\\n",
    "    --output_path /Data/Path/sparse/aligned \\\n",
    "    --database_path /Data/Path/database.db \\\n",
    "    --ref_is_gps 1 \\\n",
    "    --alignment_type ECEF \\\n",
    "    --alignment_max_error 3.0\n",
    "\n",
    "# Undistort original input images so they are as if they were taken with a pinhole camera\n",
    "!colmap image_undistorter \\\n",
    "    --image_path /Data/Path/images_raw \\\n",
    "    --input_path /Data/Path/sparse/0 \\\n",
    "    --output_path /Data/Path \\\n",
    "    --output_type=COLMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de621e",
   "metadata": {},
   "source": [
    "Let's take a closer look at the files from the provided SfM run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e583f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of directory in tree like format\n",
    "!tree ../../Data/safety_park"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc8b87",
   "metadata": {},
   "source": [
    "Now let's delve deeper into each of these files and folders.\n",
    "\n",
    "**images_raw**: Contains the original images <br>\n",
    "**images**: Contains undistorted images <br>\n",
    "**sparse**: Contains the sparse 3D reconstruction of the scene in folder 0. If COLMAP cannot register the images into 1 single scene, it will be split in to additional numbered folders. <br>\n",
    "**cameras.bin**: Camera intrinsics including camera IDs, camera models, and sensor dimensions. <br>\n",
    "**images.bin**: Camera poses and keypoints for all reconstructed images. <br>\n",
    "**points3D.bin**: The sparse 3D point cloud <br>\n",
    "To learn more these files COLMAP produces see the [COLMAP's Output Format page](https://colmap.github.io/format.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60144cc7",
   "metadata": {},
   "source": [
    "### fVDB Reality Capture\n",
    "Now that we have a SfM run we can visualize it and use it to train a Gaussian splatting scene. We will use an [fVDB](https://github.com/openvdb/fvdb-core) example project called [fVDB Reality Capture](https://github.com/openvdb/fvdb-reality-capture) for visualization, manipulation and training. fVDB is a framework for encoding and operating on sparse voxel hierarchies of features in PyTorch. Voxels are like pixels but they are cubes instead of squares, making them three dimensional. Sparse means we only have voxels in areas of our scene that are occupied, voxels that don't contain anything are not represented. fVDB Reality Capture (fRC) is toolbox for reality capture tasks built on top of fVDB. It gathers the tools required to create an Isaac Sim compatible 3D reconstruction from a set of images and uses fVDB to make the tools fast and efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c3846",
   "metadata": {},
   "source": [
    "#### Setup and Imports\n",
    "\n",
    "Before we get started, let's import the packages we need. Here's an overview of some of the important ones:\n",
    "\n",
    "* `logging`\n",
    "    - We'll use the built-in Python `logging` module, and call `logging.basicConfig()` which will cause functions within `fvdb_reality_capture` to log to stdout. You don't have to enable this, but it's useful to see what's happening under the hood.\n",
    "* `fvdb` \n",
    "    - We use `fvdb` for the underlying Gaussian splat data structure (`fvdb.GaussianSplat3d`) which provides fast and scalable rendering, and for interactive visualization (using the `fvdb.viz`) module.\n",
    "* `fvdb_reality_capture` \n",
    "    - We use this for core algorithms that reconstruct scenes from sensors help us read and process capture data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import cv2\n",
    "import fvdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import fvdb_reality_capture as frc\n",
    "\n",
    "# Let's use verbose logging to track what happens under the hood.\n",
    "# For less output set level=logging.WARN. For more set level=logging.DEBUG\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s : %(message)s\")\n",
    "\n",
    "# Initialize the fvdb.viz module for interactive 3D visualization.\n",
    "# This will spin up a small HTTP server in the background.\n",
    "fvdb.viz.init(port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f879edf",
   "metadata": {},
   "source": [
    "#### Visualize a Sfm Scene\n",
    "\n",
    "We can use fRC to view and manipulate our SfM scene since it supports loading in capture data stored in different formats into a common representation that can be easily manipulated by users. To do this, data from a capture is stored in an `fvdb_reality_capture.SfmScene` object which acts as an in-memory representation of a 3D capture. We will use this object to manipulate and visualize our SfM scene. Let's load our scene into a `SfmScene` by passing in the folder containing all the SfM output and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89251ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Safety Park SfM Scene\n",
    "sfm_scene = frc.sfm_scene.SfmScene.from_colmap(\"../../Data/safety_park\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c17268",
   "metadata": {},
   "source": [
    "Let's take a closer look at what a SfmScene scene consists of.\n",
    "* `SfmScene.cameras`: A dictionary mapping unique camera IDs to `SfmCameraMetadata` objects which contain camera parameters (e.g. projection matrices, distortion parameters). The size of this dictionary matches the number of cameras used to capture the scene (so if you scanned a scene with a pair of stereo cameras, then len(SfmScene.cameras) will be 2).\n",
    "* `SfmScene.images`: A list of SfmImageMetadata objects which contain paths to the images and optional masks, a reference to the camera (SfmCameraMetadata) used to capture each image, their camera-to-world (and inverse) transformations, and the set of 3D points visible in each image.\n",
    "* `points/points_rgb/points_err`: Numpy arrays of shape (N,3)/(N,3)/(N,) encoding known surface points in the scene, their RGB colors, and an unnormalized confidence value of the accuracy of that point. Note, N denotes the number of points here. <br>\n",
    "\n",
    "Now that we have a loaded SfmScene, let's plot some of its images, and the projected 3D points within those images. We can use this object to display our training images and project the 3D points from the sparse point cloud on to them.  We'll visualize the 3D points and cameras interactively using `fvdb.viz` and we'll plot some images with their visible points using `matplotlib`. The `fvdb.viz` module provides a high performance debug visualizer written in [vulkan](https://www.vulkan.org/). It spins up a small HTTP server which streams a visualization to a notebook or a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ac46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize an image in an SfmScene and the 3D points visible from that images\n",
    "# projected onto the image plane as blue dots.\n",
    "def plot_image_from_scene(scene: frc.sfm_scene.SfmScene, image_id: int):\n",
    "    image_meta: frc.sfm_scene.SfmPosedImageMetadata = scene.images[image_id]\n",
    "    camera_meta: frc.sfm_scene.SfmCameraMetadata = image_meta.camera_metadata\n",
    "\n",
    "    # Get the visible 3d points for this image\n",
    "    visible_points_3d: np.ndarray = scene.points[image_meta.point_indices]\n",
    "\n",
    "    # Project those points onto the image plane\n",
    "    # 1. Get the world -> camera space transform and projection matrix\n",
    "    world_to_cam_matrix: np.ndarray = image_meta.world_to_camera_matrix\n",
    "    projection_matrix: np.ndarray = camera_meta.projection_matrix\n",
    "    # 2. Transform world points to camera space\n",
    "    visible_points_3d_cam_space = world_to_cam_matrix[:3,:3] @ visible_points_3d.T + world_to_cam_matrix[:3,3:4]\n",
    "    # 3. Transform camera space coordinates to image space\n",
    "    visible_points_2d = projection_matrix @ visible_points_3d_cam_space\n",
    "    visible_points_2d /= visible_points_2d[2]\n",
    "\n",
    "    # Load the image and convert to RGB (OpenCV uses BGR by default)\n",
    "    loaded_image = cv2.imread(image_meta.image_path)\n",
    "    assert loaded_image is not None, f\"Failed to load image at {image_meta.image_path}\"\n",
    "    loaded_image = cv2.cvtColor(loaded_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plot the image and projected points\n",
    "    plt.title(f\"SfmScene Image {image_id}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(loaded_image)\n",
    "    plt.scatter(visible_points_2d[0], visible_points_2d[1], color=\"#432de9\", marker=\".\", s=2)\n",
    "\n",
    "# Visualize the SfmScene interactively in a 3D viewer using fvdb.viz.Viewer\n",
    "def visualize_sfm_scene(scene: frc.sfm_scene.SfmScene,\n",
    "                        name: str,\n",
    "                        center_scene: bool = False):\n",
    "\n",
    "    viewer_scene = fvdb.viz.get_scene(\"SfmScene Visualization\")\n",
    "    # Optionally center the scene at the origin.\n",
    "    # This is useful to visualize multiple scenes together without them being far apart.\n",
    "    if center_scene:\n",
    "        center_transform = np.eye(4)\n",
    "        center_transform[:3, 3] = -np.median(scene.points, axis=0)\n",
    "        scene = scene.apply_transformation_matrix(center_transform)\n",
    "\n",
    "    # Plot the points in the SfmScene with their colors (which are uint8 by default but the viewer\n",
    "    # expects float32 colors in [0,1]).\n",
    "    # Each point is drawn as a small sphere with a 2 pixel radius.\n",
    "    viewer_scene.add_point_cloud(\n",
    "        name=f\"{name} Points\",\n",
    "        points=scene.points,\n",
    "        colors=scene.points_rgb.astype(np.float32) / 255.0,\n",
    "        point_size=2.0)\n",
    "\n",
    "    # Plot the cameras as coordinate frames with axis length 2 units,\n",
    "    # and frustums whose distance from the origin to camera plane is 1 unit long.\n",
    "    viewer_scene.add_cameras(\n",
    "        f\"{name} Cameras\",\n",
    "        camera_to_world_matrices=scene.camera_to_world_matrices,\n",
    "        projection_matrices=scene.projection_matrices,\n",
    "        axis_length=2,\n",
    "        frustum_scale=2.5,\n",
    "    )\n",
    "\n",
    "    # Set the initial camera view to be at the position of the first posed image, in the SfmScene,\n",
    "    # looking at the center of the 3D points, with Z as up (COLMAP SfM scenes use Z as up).\n",
    "    viewer_scene.set_camera_lookat(\n",
    "        eye=scene.image_camera_positions[0],\n",
    "        center=np.zeros(3),\n",
    "        up=np.array([0, 0, 1]),  # Z is up in COLMAP SfM scenes\n",
    "    )\n",
    "\n",
    "\n",
    "# Plot three images from the scene and their visible 3D points alongside each other\n",
    "plt.figure(figsize=(25, 4.25))\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_image_from_scene(sfm_scene, 8)\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_image_from_scene(sfm_scene, 16)\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_image_from_scene(sfm_scene, 32)\n",
    "plt.show()\n",
    "\n",
    "# View the SfmScene interactively in a 3D viewer\n",
    "visualize_sfm_scene(sfm_scene, \"Raw SfmScene\", center_scene=True)\n",
    "fvdb.viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e7bc8",
   "metadata": {},
   "source": [
    "You can interact with the above viewer running on [localhost:8000](http://localhost:8000/). For zooming, hold the scroll wheel and drag the mouse up or down. Zoom out enough to see the camera positions represented in green, You can also pan with the right mouse button and rotate with the left mouse button. \n",
    "\n",
    "If you want to know more about manipulation of SfmScenes using fVDB see the [Reconstructing a Gaussian Splat Radiance Field and High Quality Mesh from a Capture\n",
    "](https://github.com/openvdb/fvdb-reality-capture/blob/main/notebooks/radiance_field_and_mesh_reconstruction.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69bff0-48a3-4b26-ae66-67edda1f24e7",
   "metadata": {},
   "source": [
    "#### Preprocess Scene\n",
    "You may have noticed the scene in the viewer is rotated with respect to the canonical axes. It's also very noisy with a lot of outlier points.\n",
    "This is because the structure-from-motion algorithm which produced this data had a lot of noisy predictions, and predicted points and cameras in a rotated coordinate frame.\n",
    "Before we reconstruct a Gaussian Splat radiance field, let's clean up our input data a bit. We'll apply the following steps:\n",
    "\n",
    " 1. Downsample the images by a factor of 2 to speed up Gaussian Splat optimization (loading big images can be time consuming), \n",
    " 2. Normalize the scene to an east-north-up coordinate system.\n",
    " 3. Remove outlier points below the bottom 3rd and above top 97th percentiles along the X, Y, and Z, axis.\n",
    " 4. Remove any images with fewer than 50 visible points (these images are likely to have bad pose estimates)\n",
    "\n",
    "`fvdb_reality_capture` makes this kind of cleanup easy, efficient, and seamless using the `transforms` module. \n",
    "Transforms are classes which define a transformation of an `SfmScene`. They inherit from `fvdb_reality_capture.BaseTransform`, and their `__call__` operator accepts an `SfmScene` as input and produces a new `SfmScene` as output. Let's look at some code and visualizations and then dive into how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab53626-26ba-46fd-82e3-4d020c38fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing using transforms\n",
    "import fvdb_reality_capture.transforms as fvtransforms\n",
    "cleanup_and_resize_transform = fvtransforms.Compose(\n",
    "    # Downsample images by factor of 2\n",
    "    fvtransforms.DownsampleImages(image_downsample_factor=2, image_type=\"jpg\", rescaled_jpeg_quality=95),\n",
    "    # Normalize the scene to an est-north-up coordinate system.\n",
    "    fvtransforms.NormalizeScene(normalization_type=\"ecef2enu\"),\n",
    "    # Remove outlier points\n",
    "    fvtransforms.PercentileFilterPoints(percentile_min=3.0, percentile_max=97.0),\n",
    "    # filters outlier points, and removes images with too few points.\n",
    "    fvtransforms.FilterImagesWithLowPoints(min_num_points=50),\n",
    ")\n",
    "\n",
    "# Apply the transforms\n",
    "sfm_scene = cleanup_and_resize_transform(sfm_scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6b6d9",
   "metadata": {},
   "source": [
    "#### Train a Gaussian Splatting Scene \n",
    "In this section we will walk through training a Gaussian splatting scene from a set of images and a SfM output, but let's start with what a Gaussian splatting scene is.\n",
    "\n",
    "[3D Gaussian Splatting](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/) is the dominant radiance field representation. A [radiance field](https://radiancefields.com/) is a 3D scene representation that specifies the color of a point along a given view direction. Radiance fields enable high fidelity visualization of 3D captures with realistic lighting effects. 3D Gaussian splats encode a radiance field function as a sum of 3D Gaussian functions or splats, parameterized by their means (positions), rotations, and scales. Additionally each splat has an opacity value and a set of spherical harmonics that map the direction in which a Gaussian is viewed to a color. Each scene is made up of thousands to millions of these splats. Think of them as similar to voxels, or points in a point cloud. We start with splats in the same positions as the points in the sparse point cloud from our SfM run. During training splats are added, removed, split, and changed until the Gaussian splatting scene is capable of producing renders that are nearly identical to the training images. \n",
    "<div>\n",
    "<img src=\"https://fvdb-data.s3.us-east-2.amazonaws.com/fvdb-reality-capture/doc_figures/fvdb_gs_opt.jpg\" width=\"80%\"/>\n",
    "</div>\n",
    "\n",
    "Now that we have an understanding of what a Gaussian splatting scene is, let's train our own. The core API for Gaussian Splat reconstruction is the `fvdb_reality_capture.GaussianSplatReconstruction` class. It accepts an input `SfmScene` and optional config parameters, and produces an `fvdb.GaussianSplat3d` reconstructing the scene. This functionally is also written in `/fvdb_reality_capture/cli/frgs/_reconstruct.py`. We will show how to use both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef9b0d-e596-4ca7-b768-5e2426936e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config so we can edit various parameters\n",
    "cfg = frc.radiance_fields.GaussianSplatReconstructionConfig()\n",
    "cfg.eval_at_percent = [100]  # save evaluation for the end of training\n",
    "cfg.save_at_percent = [100]  # Don't save the model until the end of training\n",
    "cfg.refine_every_epoch = 4.5  # How often to refine Gaussians during optimization\n",
    "cfg.pose_opt_start_epoch = 100  # Epoch at which we start optimizing camera positions\n",
    "\n",
    "# Create a GaussianSplatReconstruction runner\n",
    "runner = frc.radiance_fields.GaussianSplatReconstruction.from_sfm_scene(\n",
    "    config=cfg,  # The reconstruction config\n",
    "    sfm_scene=sfm_scene,  # The SfM scene\n",
    "    use_every_n_as_val=-1,  # Don't leave any images for validation\n",
    ")\n",
    "\n",
    "# Train\n",
    "runner.optimize()\n",
    "\n",
    "# Save results\n",
    "runner.save_ply(\"../../Results/safety_park/reconstruction.ply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20085bc",
   "metadata": {},
   "source": [
    "Higher resolution images and a larger quantity of images will cause training to take longer. We have pretrained a high fidelity splat for you to use. To stop the training, click the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19024a12",
   "metadata": {},
   "source": [
    "Our optimizer was created with several different parameters. They effect when we save the model, how often the splats are refined, and more. There are many more parameters you can vary for training beyond these few. We will use the command-line interface (CLI) tool called `frgs` for easy usage of Gaussian splatting tools. To learn more about `frgs` please see [Reconstruction on the CLI with frgs\n",
    "](https://openvdb.github.io/fvdb-reality-capture/tutorials/frgs.html) documentation. Now we'll run the reconstruct script with the help flag to see all the possible arguments and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!frgs reconstruct --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b0baa",
   "metadata": {},
   "source": [
    "Trying changing and adding parameters to see how they effect training. To stop the run early you can click the square button at the top of the notebook to interrupt the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing and adding parameters\n",
    "!frgs reconstruct ../../Data/safety_park -o ../../Results/safety_park/params_change.ply --cfg.max-epochs 100 --cfg.eval-at-percent 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc81f0",
   "metadata": {},
   "source": [
    "#### Visualize a Gaussian Splatting Scene\n",
    "A major benefit of a 3D radiance fields is that we can render them continuously from any point in space in real time. Let's interactively visualize the reconstructed Gaussian Splat, examining the result from novel, freeform viewpoints. The viewer in `fvdb.viz` makes this straightforward by letting us visualize `fvdb.GaussianSplat3d` objects directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a trained splat model from a saved PLY file\n",
    "pretrained_splat_path = \"../../Data/isaac_files/safety_park_splats.ply\"\n",
    "model, metadata = fvdb.GaussianSplat3d.from_ply(pretrained_splat_path, device=\"cuda\")\n",
    "\n",
    "# Clear previous contents from the viewer\n",
    "# viewer.clear()\n",
    "\n",
    "# Add our splat model to the viewer\n",
    "scene = fvdb.viz.get_scene(\"Gaussian Splat Model Visualization\")\n",
    "scene.add_gaussian_splat_3d(\"Reconstructed Gaussian Splat Radiance Field\", model)\n",
    "\n",
    "scene.add_cameras(\n",
    "    \"Input Cameras\",\n",
    "    camera_to_world_matrices=sfm_scene.camera_to_world_matrices,\n",
    "    projection_matrices=sfm_scene.projection_matrices,\n",
    "    axis_length=2,\n",
    "    frustum_scale=2.5,\n",
    ")\n",
    "\n",
    "# Set up the viewer's initial camera to be positioned at the first camera in the SfmScene\n",
    "# looking at the center of the scene. This should give a good initial view of the model.\n",
    "camera_position = sfm_scene.images[0].origin\n",
    "camera_lookat_point = model.means.mean(dim=0)\n",
    "scene.set_camera_lookat(eye=camera_position, center=camera_lookat_point, up=(0, 0, 1)) # Colmap uses Z as up\n",
    "fvdb.viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26833e2-2bcb-47de-aa8f-bc36760e4b68",
   "metadata": {},
   "source": [
    "#### Convert to a Mesh\n",
    "In order for a robot to move around in Isaac Sim there needs to be a surface it can interact with. Gaussian splatting scenes cannot represent a solid surface in Isaac Sim, so we need to convert our splats to a mesh capable of acting as a collider. `fvdb_reality_capture.tools` provides `mesh_from_splats_dlnr` which will use a foundation model to compute high quality depth maps from images rendered from our Gaussian splat. \n",
    "The method works by rendering stereo pairs from the splat scene, and uses the [DLNR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf) foundation model to perform stereo depth estimation. The DLNR model is a high-frequency stereo matching network that computes optical flow and disparity maps between two images.\n",
    "The we can fuse the depth estimations into a truncated signed distance field (TSDF) using the [TSDF](https://www.microsoft.com/en-us/research/publication/kinectfusion-real-time-3d-reconstruction-and-interaction-using-a-moving-depth-camera/) fusion algorithm. TSDF fusion accumulates noisy depth maps into a voxel grid, to approximate a signed distance field near the surface of the object. `fvdb_reality_capture` makes use of `fvdb-core` to provide a high performance implementation of TSDF integration on sparse voxels. A mesh can then be extracted from the TSDF using the marching cubes algorithm implemented in `fvdb.marching_cubes.marching_cubes`. This allows us to generate meshes from Gaussian splats at very high resolutions on the GPU while using minimal memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8132b-87c2-406b-b1c6-43a32d364dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvdb_reality_capture.tools import mesh_from_splats\n",
    "import point_cloud_utils as pcu\n",
    "\n",
    "# The truncation margin determines the width of the narrow band around the surface in which we compute the TSDF.\n",
    "# A larger margin will produce coarser voxels, while a smaller margin will produce finer voxels but may miss some surface details.\n",
    "# Here we pick a truncation margin of 0.25 world units in our scene.\n",
    "truncation_margin = 0.5\n",
    "\n",
    "# load the Gaussian splatting scene\n",
    "pretrained_splat_path = \"../../Data/safety_park_half/pretrained_splat.ply\"\n",
    "model, metadata = fvdb.GaussianSplat3d.from_ply(pretrained_splat_path, device=\"cuda\")\n",
    "# This function returns a tensor of vertices, faces, and colors for the mesh.\n",
    "# The vertices have shape (num_vertices, 3), the faces have shape (num_faces, 3),\n",
    "# and the colors have shape (num_vertices, 3). The colors are in the range [0, 1].\n",
    "v, f, c = mesh_from_splats(model, sfm_scene.camera_to_world_matrices, sfm_scene.projection_matrices, sfm_scene.image_sizes, truncation_margin)\n",
    "\n",
    "# Save the mesh as a PLY file for viewing in external tools using point_cloud_utils (https://fwilliams.info/point-cloud-utils/) [3]\n",
    "pcu.save_mesh_vfc(\"../../Results/safety_park_isaac_files/safety_park_mesh.ply\", v.cpu().numpy(), f.cpu().numpy(), c.cpu().numpy())\n",
    "\n",
    "print(f\"Reconstructed mesh with {v.shape[0]:,} vertices and {f.shape[0]:,} faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0649f68",
   "metadata": {},
   "source": [
    "#### Create Isaac Sim Ready Files\n",
    "\n",
    "As we saw in viewer, splats end up beyond the main focus of our scene. The training process tries to reconstruct the all the details in the images, even details far in the horizon. This can lead to messy looking edges. We will crop both the mesh and splats to focus on the center of our scene using **create_isaac_sim_ready_files.py**. There are also a few other things this script does to make our mesh and splats compatible with Isaac Sim.\n",
    "\n",
    "* Coordinate System: Isaac Sim uses different world axes coordinates than COLMAP, it assumes Z+ is up rather than -Y, so we will rotate the scene accordingly. \n",
    "* Water Tight: To ensure our mesh will act as a collider in Isaac Sim we will make it water tight. This will fill in holes in the mesh and smooth it out.\n",
    "* Format Mesh: We will convert the mesh from a PLY to an OBJ, a file type compatible Isaac Sim.\n",
    "* Format Splats: Isaac Sim doesn't understand a Gaussian splatting scene in PLY format, so we will have to convert ours into something Isaac Sim can render. USDs are commonly used with Isaac Sim, so we will convert our scene to a Universal Scene Description Zip (USDZ), a compressed version of USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac446a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script to make Isaac Sim Ready Files\n",
    "# --input_splat: Location of Gaussian splatting scene\n",
    "# --input_mesh: Location of mesh (PLY)\n",
    "# --output_path: Where to save the OBJ and USDZ (no file extension)\n",
    "# --bbox: Box to crop to\n",
    "# --resolution: How detailed our mesh will be. Increase if you want more faces and vertices.\n",
    "!python ../scripts/create_isaac_ready_files.py \\\n",
    "--input-splat ../../Data/safety_park_half/pretrained_splat.ply \\\n",
    "--input-mesh ../../Data/safety_park_half/pretrained_mesh.ply \\\n",
    "--output-path ../../Results/safety_park_isaac_files/safety_park_cropped \\\n",
    "--bbox -100 -70 -20 110 90 20 \\\n",
    "--resolution 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d574cfe",
   "metadata": {},
   "source": [
    "## Part 2: Creating an Isaac Sim Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a007239",
   "metadata": {},
   "source": [
    "Isaac Sim 5.0 and above includes [NuRec (Neural Reconstruction) rendering](https://docs.isaacsim.omniverse.nvidia.com/5.0.0/assets/usd_assets_nurec.html), adding the functionality to render Gaussian splatting scenes among other neural volume methods. In this section we will go through the process of creating an environment from a Gaussian splatting scene and mesh. Then, using Isaac Lab, we will walk a [Spot](https://bostondynamics.com/products/spot/) robot around the scene. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0edac3",
   "metadata": {},
   "source": [
    "### Running Isaac Sim\n",
    "To run Isaac Sim you need to locate the isaacsim folder, navigate to the release subdirectory, and run the isaac-sim.sh file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../isaacsim/_build/linux-x86_64/release && ./isaac-sim.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83139cb0",
   "metadata": {},
   "source": [
    "After a few moments Isaac Sim will open and you should see the following window. \n",
    "\n",
    "<img src=\"./images/Isaac_sim_launch.png\" width=\"800\" style=\"display:block; margin:auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86dd3b8",
   "metadata": {},
   "source": [
    "### Import the Assets\n",
    "Once Isaac Sim has launched you can import the Gaussian splatting scene and mesh made in [Part 1](#part-1-creating-a-digital-twin-with-gaussian-splatting).\n",
    "1. In the content tab navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files**, inside this folder is the **safety_park_mesh_res_50000.obj**, drag the file into the stage window on the right.\n",
    "2. In the content window, still in **/home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files**, drag **safety_park_splats.usdz** into the stage window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc35737-d331-485f-9bbc-3bd3462dd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Isaac Sim file import demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/import_mesh_and_splats.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a59d1",
   "metadata": {},
   "source": [
    "Now the 3D Gaussian scene and the mesh should be overlapping in the viewer. The mesh will be very small at first, let's see how we can fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58b013",
   "metadata": {},
   "source": [
    "### Scene Setup\n",
    "Let's setup our scene so its ready for a robot.\n",
    "1. In the **Stage** tab click the **safety_park_mesh_res_50000** xform.\n",
    "2. In the **Property** tab, under **Transform**, change the **Scale:unitsResolve** to 1.0 for X, Y and Z.\n",
    "4. Back in the stage right click the **safety_park_mesh_res_50000** xform and select **Add > Physics > Colliders Preset**\n",
    "    * This makes it so other objects collied with our mesh instead passing right through it.\n",
    "5. Click the eye icon next to the **safety_park_mesh_res_50000** in the **Stage** window to hide the mesh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373d891-d389-4241-bba4-57dff7627319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the align splats and mesh in Isaac Sim demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/scene_setup.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db3117",
   "metadata": {},
   "source": [
    "Now the mesh and the 3D Gaussian scene are aligned. We also hid the mesh from view. It will still act as a collider but now we can use just high resolution splats for the visualization of Safety Park."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd9cfb",
   "metadata": {},
   "source": [
    "### Optional: Splat Color Editing\n",
    "Currently, lights don't interact with the Gaussian splatting scene, just the mesh. Since changing the strength of lights in the environment will have no effect on the look of the splats, we can artificially change the lighting by changing the emissive color values of the Gaussian splatting scene.\n",
    "1. In the **Stage** tab, select the **safety_park_splat > gauss > gauss > emissive_color_field** asset.\n",
    "2. In the **Property** tab, under **Raw USD Proprieties**\n",
    "    * Change Z in **emissive_color_field.omni:nurec:ccmB** to .7\n",
    "    * Change Y in **emissive_color_field.omni:nurec:ccmG** to .7\n",
    "    * Change X in **emissive_color_field.omni:nurec:ccmR** to .7\n",
    "\n",
    "These values represent the amount the strength of emission for each of the 3 color channels. Be decreasing them all to 0.7 the splat looks less bright. Experiment with changing the values. Changing them non uniformly will result in changes to the color of the scene, rather than just the intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029d364-16d3-4b7f-89e7-7addc2204794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the adjust splat colors in Isaac Sim demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/color.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b3298",
   "metadata": {},
   "source": [
    "### Save Scene\n",
    "To use the scene we created with Isaac Lab and a Spot robot we need to save it.\n",
    "1. Navigate to **File > Save As...**\n",
    "2. In the file browser that pops up navigate to **/home/nvidia/Reconstructing_Outdoor_Environments/Result/safety_park_isaac_files**\n",
    "3. Save the scene as **isaac_sim_scene.usd**.\n",
    "4. Exit out of Isaac Sim using the close button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d199a0-e5bb-4920-884e-9db7e1f8e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the save scene as USD demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/save_usd.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623ba96",
   "metadata": {},
   "source": [
    "### Isaac Lab and Robot Locomotion\n",
    "We can now use Isaac Lab and Isaac Sim to teleoperate a quadruped robot model (Boston Dynamics Spot) around our saved scene using a keyboard or a gaming controller. A locomotion policy was trained for this robot model using reinforcement learning in Isaac Lab, and we can inference this policy to translate velocity commands from the keyboard or controller into the joint-level actions required for the robot to walk. We need to launch Isaac Lab from outside our fVDB Python environment, so lets open a terminal.\n",
    "\n",
    "1. Open a terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gnome-terminal --working-directory=/home/nvidia/Reconstructing_Outdoor_Environments/Code/IsaacLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da807f7d",
   "metadata": {},
   "source": [
    "2. To ensure Isaac Lab has access to as much GPU as possible, lets kill the Jupyter Notebook. Go to **File > Shut Down**. When promoted confirm you want to shut down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29df6a7",
   "metadata": {},
   "source": [
    "3. Isaac Lab needs to be ran outside of conda Python environments, including the fVDB environment and the base environment. Copy and paste the following command into the terminal so we are no longer working inside a conda environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681478d-203d-468b-be7d-245449a14fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate conda environments\n",
    "conda deactivate && conda deactivate && conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bec7bc",
   "metadata": {},
   "source": [
    "4. Now we can open our scene using Isaac Lab. It will launch a with Spot robot and its accompanying locomotion policy. Copy and paste the following command into the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827abd6-9af4-4c68-9427-1d0e782fab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Isaac Lab to launch Isaac Sim with with a Spot robot and location policy in Isaac Sim\n",
    "./isaaclab.sh -p /home/nvidia/Reconstructing_Outdoor_Environments/Code/robo_rl2/policy_inference_in_usd_safetypark.py --checkpoint /home/nvidia/Reconstructing_Outdoor_Environments/Code/robo_rl2/policy.pt --keyboard --terrain_usd /home/nvidia/Reconstructing_Outdoor_Environments/Data/isaac_files/safety_park_isaac_scene.usd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e38c2",
   "metadata": {},
   "source": [
    "5. Use the arrow keys to move the Spot robot around. You can use the **X** and **Z** keys to control the yaw and turn the robot. Try running into objects to see how the collider is stopping the Spot.\n",
    "6. The camera should follow the Spot robot around the scene as you explore Safety Park. You can change the position of the camera in the **Isaac Lab** tab on the right. Try changing the **Camera Eye** and **Camera Target** to see how it changes your view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d16eb-db7c-43d8-91fb-9c56676e9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the Isaac Lab demo video\n",
    "from IPython.display import Video\n",
    "Video(\"./images/isaac_lab.webm\", width=800, embed=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd019c8d",
   "metadata": {},
   "source": [
    "Congratulations! You've completed this course, *Reconstructing Outdoor Environments for Physical AI Simulation with 3D Gaussian Splatting in NVIDIA Isaac Sim*. You can now create a 3D Gaussian Splatting reconstuction of a scene and use that scene with NVIDIA Omniverse in Isaac Sim and Isaac Lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fvdb",
   "language": "python",
   "name": "fvdb-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
